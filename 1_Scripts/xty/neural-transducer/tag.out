INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: seed - 0
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: train - ['../../../dataset/xty.train.tsv']
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: dev - ['../../../dataset/xty.dev.tsv']
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: test - ['../../../dataset/xty.PREP.tsv']
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: model - 'checkpoints/sig22/tagtransformer/xty'
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: load - ''
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: bs - 400
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: epochs - 100
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: max_steps - 0
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: warmup_steps - 100
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: total_eval - 50
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: optimizer - <Optimizer.adam: 'adam'>
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: scheduler - <Scheduler.warmupinvsqr: 'warmupinvsqr'>
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: lr - 0.001
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: min_lr - 1e-05
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: momentum - 0.9
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: beta1 - 0.9
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: beta2 - 0.98
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: estop - 1e-08
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: cooldown - 0
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: patience - 0
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: discount_factor - 0.5
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: max_norm - 1.0
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: gpuid - [0]
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: loglevel - 'info'
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: saveall - False
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: shuffle - True
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: cleanup_anyway - True
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: dataset - <Data.sigmorphon17task1: 'sigmorphon17task1'>
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: max_seq_len - 128
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: max_decode_len - 32
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: decode_beam_size - 5
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: init - ''
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: dropout - 0.3
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: embed_dim - 256
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: nb_heads - 4
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: src_layer - 10
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: trg_layer - 10
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: src_hs - 1024
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: trg_hs - 1024
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: label_smooth - 0.1
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: tie_trg_embed - False
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: arch - <Arch.tagtransformer: 'tagtransformer'>
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: nb_sample - 2
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: wid_siz - 11
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: indtag - False
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: decode - <Decode.greedy: 'greedy'>
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: mono - False
INFO - 03/12/24 22:04:00 - 0:00:00 - command line argument: bestacc - True
INFO - 03/12/24 22:04:00 - 0:00:00 - src vocab size 45
INFO - 03/12/24 22:04:00 - 0:00:00 - trg vocab size 37
INFO - 03/12/24 22:04:00 - 0:00:00 - src vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', ' ', '(', ')', ',', '-', '/', '=', 'a', 'b', 'c', 'd', 'e', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 's', 't', 'u', 'w', 'x', 'y', '²', '³', '¹', 'ñ', 'ʔ', '⁴', 'IPFV', 'IRR', 'LGSPEC1', 'LGSPEC2', 'NEG', 'PFV', 'STAT', 'V']
INFO - 03/12/24 22:04:00 - 0:00:00 - trg vocab ['<PAD>', '<BOS>', '<EOS>', '<UNK>', ' ', '(', ')', ',', '-', '/', '=', 'a', 'b', 'c', 'd', 'e', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 's', 't', 'u', 'w', 'x', 'y', '²', '³', '¹', 'ñ', 'ʔ', '⁴']
INFO - 03/12/24 22:04:01 - 0:00:00 - model: TagTransformer(
                                       (src_embed): Embedding(45, 256, padding_idx=0)
                                       (trg_embed): Embedding(37, 256, padding_idx=0)
                                       (position_embed): SinusoidalPositionalEmbedding()
                                       (encoder): TransformerEncoder(
                                         (layers): ModuleList(
                                           (0): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (1): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (2): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (3): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (4): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (5): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (6): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (7): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (8): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (9): TransformerEncoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                         )
                                         (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                       )
                                       (decoder): TransformerDecoder(
                                         (layers): ModuleList(
                                           (0): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (1): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (2): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (3): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (4): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (5): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (6): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (7): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (8): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                           (9): TransformerDecoderLayer(
                                             (self_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (multihead_attn): MultiheadAttention(
                                               (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)
                                             )
                                             (linear1): Linear(in_features=256, out_features=1024, bias=True)
                                             (dropout): Dropout(p=0.3, inplace=False)
                                             (linear2): Linear(in_features=1024, out_features=256, bias=True)
                                             (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                             (activation_dropout): Dropout(p=0.3, inplace=False)
                                           )
                                         )
                                         (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
                                       )
                                       (final_out): Linear(in_features=256, out_features=37, bias=True)
                                       (dropout): Dropout(p=0.3, inplace=False)
                                       (special_embeddings): Embedding(2, 256)
                                     )
INFO - 03/12/24 22:04:01 - 0:00:00 - number of parameter 18464037
INFO - 03/12/24 22:04:02 - 0:00:02 - maximum training 700 steps (100 epochs)
INFO - 03/12/24 22:04:02 - 0:00:02 - evaluate every 2 epochs
INFO - 03/12/24 22:04:02 - 0:00:02 - At 0-th epoch with lr 0.000000.
INFO - 03/12/24 22:04:04 - 0:00:04 - Running average train loss is 3.7722532749176025 at epoch 0
INFO - 03/12/24 22:04:04 - 0:00:04 - At 1-th epoch with lr 0.000070.
INFO - 03/12/24 22:04:06 - 0:00:05 - Running average train loss is 3.1029015949794223 at epoch 1
INFO - 03/12/24 22:04:06 - 0:00:05 - At 2-th epoch with lr 0.000140.
INFO - 03/12/24 22:04:07 - 0:00:07 - Running average train loss is 2.774021863937378 at epoch 2
INFO - 03/12/24 22:04:07 - 0:00:07 - Average dev loss is 2.4648096561431885 at epoch 2
INFO - 03/12/24 22:04:07 - 0:00:07 - dev accuracy is 0.0 at epoch 2
INFO - 03/12/24 22:04:07 - 0:00:07 - dev average edit distance is 8.1905 at epoch 2
INFO - 03/12/24 22:04:08 - 0:00:08 - At 3-th epoch with lr 0.000210.
INFO - 03/12/24 22:04:09 - 0:00:09 - Running average train loss is 2.3518734659467424 at epoch 3
INFO - 03/12/24 22:04:09 - 0:00:09 - At 4-th epoch with lr 0.000280.
INFO - 03/12/24 22:04:11 - 0:00:10 - Running average train loss is 2.0690580095563615 at epoch 4
INFO - 03/12/24 22:04:11 - 0:00:10 - Average dev loss is 1.894805908203125 at epoch 4
INFO - 03/12/24 22:04:11 - 0:00:11 - dev accuracy is 0.0 at epoch 4
INFO - 03/12/24 22:04:11 - 0:00:11 - dev average edit distance is 7.119 at epoch 4
INFO - 03/12/24 22:04:12 - 0:00:12 - At 5-th epoch with lr 0.000350.
INFO - 03/12/24 22:04:13 - 0:00:13 - Running average train loss is 1.8862390858786446 at epoch 5
INFO - 03/12/24 22:04:13 - 0:00:13 - At 6-th epoch with lr 0.000420.
INFO - 03/12/24 22:04:15 - 0:00:14 - Running average train loss is 1.7540967294148035 at epoch 6
INFO - 03/12/24 22:04:15 - 0:00:14 - Average dev loss is 1.617264747619629 at epoch 6
INFO - 03/12/24 22:04:15 - 0:00:15 - dev accuracy is 0.0 at epoch 6
INFO - 03/12/24 22:04:15 - 0:00:15 - dev average edit distance is 5.746 at epoch 6
INFO - 03/12/24 22:04:16 - 0:00:15 - At 7-th epoch with lr 0.000490.
INFO - 03/12/24 22:04:17 - 0:00:17 - Running average train loss is 1.6175626856940133 at epoch 7
INFO - 03/12/24 22:04:17 - 0:00:17 - At 8-th epoch with lr 0.000560.
INFO - 03/12/24 22:04:19 - 0:00:18 - Running average train loss is 1.4804426091057914 at epoch 8
INFO - 03/12/24 22:04:19 - 0:00:18 - Average dev loss is 1.4070990085601807 at epoch 8
INFO - 03/12/24 22:04:19 - 0:00:19 - dev accuracy is 11.1111 at epoch 8
INFO - 03/12/24 22:04:19 - 0:00:19 - dev average edit distance is 3.246 at epoch 8
INFO - 03/12/24 22:04:20 - 0:00:19 - At 9-th epoch with lr 0.000630.
INFO - 03/12/24 22:04:21 - 0:00:21 - Running average train loss is 1.340621794973101 at epoch 9
INFO - 03/12/24 22:04:21 - 0:00:21 - At 10-th epoch with lr 0.000700.
INFO - 03/12/24 22:04:23 - 0:00:22 - Running average train loss is 1.2596502985273088 at epoch 10
INFO - 03/12/24 22:04:23 - 0:00:22 - Average dev loss is 1.2068521976470947 at epoch 10
INFO - 03/12/24 22:04:23 - 0:00:22 - dev accuracy is 32.5397 at epoch 10
INFO - 03/12/24 22:04:23 - 0:00:22 - dev average edit distance is 2.1905 at epoch 10
INFO - 03/12/24 22:04:23 - 0:00:23 - At 11-th epoch with lr 0.000770.
INFO - 03/12/24 22:04:25 - 0:00:24 - Running average train loss is 1.160700968333653 at epoch 11
INFO - 03/12/24 22:04:25 - 0:00:24 - At 12-th epoch with lr 0.000840.
INFO - 03/12/24 22:04:26 - 0:00:26 - Running average train loss is 1.064756921359471 at epoch 12
INFO - 03/12/24 22:04:26 - 0:00:26 - Average dev loss is 1.1134010553359985 at epoch 12
INFO - 03/12/24 22:04:26 - 0:00:26 - dev accuracy is 46.0317 at epoch 12
INFO - 03/12/24 22:04:26 - 0:00:26 - dev average edit distance is 1.5873 at epoch 12
INFO - 03/12/24 22:04:27 - 0:00:26 - At 13-th epoch with lr 0.000910.
INFO - 03/12/24 22:04:28 - 0:00:28 - Running average train loss is 1.002360233238765 at epoch 13
INFO - 03/12/24 22:04:28 - 0:00:28 - At 14-th epoch with lr 0.000980.
INFO - 03/12/24 22:04:30 - 0:00:29 - Running average train loss is 0.9489905408450535 at epoch 14
INFO - 03/12/24 22:04:30 - 0:00:29 - Average dev loss is 1.0294890403747559 at epoch 14
INFO - 03/12/24 22:04:30 - 0:00:30 - dev accuracy is 57.9365 at epoch 14
INFO - 03/12/24 22:04:30 - 0:00:30 - dev average edit distance is 1.1984 at epoch 14
INFO - 03/12/24 22:04:30 - 0:00:30 - At 15-th epoch with lr 0.000976.
INFO - 03/12/24 22:04:32 - 0:00:31 - Running average train loss is 0.9064762166568211 at epoch 15
INFO - 03/12/24 22:04:32 - 0:00:31 - At 16-th epoch with lr 0.000945.
INFO - 03/12/24 22:04:33 - 0:00:33 - Running average train loss is 0.8803637027740479 at epoch 16
INFO - 03/12/24 22:04:33 - 0:00:33 - Average dev loss is 0.9470935463905334 at epoch 16
INFO - 03/12/24 22:04:34 - 0:00:33 - dev accuracy is 61.1111 at epoch 16
INFO - 03/12/24 22:04:34 - 0:00:33 - dev average edit distance is 0.9762 at epoch 16
INFO - 03/12/24 22:04:34 - 0:00:33 - At 17-th epoch with lr 0.000917.
INFO - 03/12/24 22:04:35 - 0:00:35 - Running average train loss is 0.860904438155038 at epoch 17
INFO - 03/12/24 22:04:35 - 0:00:35 - At 18-th epoch with lr 0.000891.
INFO - 03/12/24 22:04:37 - 0:00:36 - Running average train loss is 0.8431306992258344 at epoch 18
INFO - 03/12/24 22:04:37 - 0:00:36 - Average dev loss is 0.9326822757720947 at epoch 18
INFO - 03/12/24 22:04:37 - 0:00:37 - dev accuracy is 58.7302 at epoch 18
INFO - 03/12/24 22:04:37 - 0:00:37 - dev average edit distance is 0.9524 at epoch 18
INFO - 03/12/24 22:04:38 - 0:00:37 - At 19-th epoch with lr 0.000867.
INFO - 03/12/24 22:04:39 - 0:00:39 - Running average train loss is 0.8294748749051776 at epoch 19
INFO - 03/12/24 22:04:39 - 0:00:39 - At 20-th epoch with lr 0.000845.
INFO - 03/12/24 22:04:40 - 0:00:40 - Running average train loss is 0.8188520755086627 at epoch 20
INFO - 03/12/24 22:04:40 - 0:00:40 - Average dev loss is 0.9308347702026367 at epoch 20
INFO - 03/12/24 22:04:41 - 0:00:40 - dev accuracy is 61.1111 at epoch 20
INFO - 03/12/24 22:04:41 - 0:00:40 - dev average edit distance is 0.9524 at epoch 20
INFO - 03/12/24 22:04:41 - 0:00:41 - At 21-th epoch with lr 0.000825.
INFO - 03/12/24 22:04:42 - 0:00:42 - Running average train loss is 0.8086244804518563 at epoch 21
INFO - 03/12/24 22:04:42 - 0:00:42 - At 22-th epoch with lr 0.000806.
INFO - 03/12/24 22:04:44 - 0:00:43 - Running average train loss is 0.7995074731963021 at epoch 22
INFO - 03/12/24 22:04:44 - 0:00:43 - Average dev loss is 0.9140390157699585 at epoch 22
INFO - 03/12/24 22:04:44 - 0:00:44 - dev accuracy is 65.873 at epoch 22
INFO - 03/12/24 22:04:44 - 0:00:44 - dev average edit distance is 0.881 at epoch 22
INFO - 03/12/24 22:04:45 - 0:00:44 - At 23-th epoch with lr 0.000788.
INFO - 03/12/24 22:04:46 - 0:00:46 - Running average train loss is 0.790510186127254 at epoch 23
INFO - 03/12/24 22:04:46 - 0:00:46 - At 24-th epoch with lr 0.000772.
INFO - 03/12/24 22:04:47 - 0:00:47 - Running average train loss is 0.782691981111254 at epoch 24
INFO - 03/12/24 22:04:47 - 0:00:47 - Average dev loss is 0.8777695894241333 at epoch 24
INFO - 03/12/24 22:04:48 - 0:00:47 - dev accuracy is 70.6349 at epoch 24
INFO - 03/12/24 22:04:48 - 0:00:47 - dev average edit distance is 0.8492 at epoch 24
INFO - 03/12/24 22:04:48 - 0:00:48 - At 25-th epoch with lr 0.000756.
INFO - 03/12/24 22:04:50 - 0:00:49 - Running average train loss is 0.7796474695205688 at epoch 25
INFO - 03/12/24 22:04:50 - 0:00:49 - At 26-th epoch with lr 0.000741.
INFO - 03/12/24 22:04:51 - 0:00:51 - Running average train loss is 0.772523318018232 at epoch 26
INFO - 03/12/24 22:04:51 - 0:00:51 - Average dev loss is 0.8766310811042786 at epoch 26
INFO - 03/12/24 22:04:51 - 0:00:51 - dev accuracy is 73.8095 at epoch 26
INFO - 03/12/24 22:04:51 - 0:00:51 - dev average edit distance is 0.754 at epoch 26
INFO - 03/12/24 22:04:52 - 0:00:51 - At 27-th epoch with lr 0.000727.
INFO - 03/12/24 22:04:53 - 0:00:53 - Running average train loss is 0.7685420938900539 at epoch 27
INFO - 03/12/24 22:04:53 - 0:00:53 - At 28-th epoch with lr 0.000714.
INFO - 03/12/24 22:04:55 - 0:00:54 - Running average train loss is 0.7697108813694545 at epoch 28
INFO - 03/12/24 22:04:55 - 0:00:54 - Average dev loss is 0.8731294870376587 at epoch 28
INFO - 03/12/24 22:04:55 - 0:00:54 - dev accuracy is 67.4603 at epoch 28
INFO - 03/12/24 22:04:55 - 0:00:54 - dev average edit distance is 0.9286 at epoch 28
INFO - 03/12/24 22:04:55 - 0:00:55 - At 29-th epoch with lr 0.000702.
INFO - 03/12/24 22:04:57 - 0:00:56 - Running average train loss is 0.7630356550216675 at epoch 29
INFO - 03/12/24 22:04:57 - 0:00:56 - At 30-th epoch with lr 0.000690.
INFO - 03/12/24 22:04:58 - 0:00:58 - Running average train loss is 0.7589585951396397 at epoch 30
INFO - 03/12/24 22:04:58 - 0:00:58 - Average dev loss is 0.8706797361373901 at epoch 30
INFO - 03/12/24 22:04:58 - 0:00:58 - dev accuracy is 69.0476 at epoch 30
INFO - 03/12/24 22:04:58 - 0:00:58 - dev average edit distance is 0.8254 at epoch 30
INFO - 03/12/24 22:04:59 - 0:00:58 - At 31-th epoch with lr 0.000679.
INFO - 03/12/24 22:05:00 - 0:01:00 - Running average train loss is 0.7538864868027824 at epoch 31
INFO - 03/12/24 22:05:00 - 0:01:00 - At 32-th epoch with lr 0.000668.
INFO - 03/12/24 22:05:02 - 0:01:01 - Running average train loss is 0.7528499024254935 at epoch 32
INFO - 03/12/24 22:05:02 - 0:01:01 - Average dev loss is 0.8710941076278687 at epoch 32
INFO - 03/12/24 22:05:02 - 0:01:02 - dev accuracy is 69.8413 at epoch 32
INFO - 03/12/24 22:05:02 - 0:01:02 - dev average edit distance is 0.8254 at epoch 32
INFO - 03/12/24 22:05:02 - 0:01:02 - At 33-th epoch with lr 0.000658.
INFO - 03/12/24 22:05:04 - 0:01:03 - Running average train loss is 0.7496752568653652 at epoch 33
INFO - 03/12/24 22:05:04 - 0:01:03 - At 34-th epoch with lr 0.000648.
INFO - 03/12/24 22:05:05 - 0:01:05 - Running average train loss is 0.7476558344704765 at epoch 34
INFO - 03/12/24 22:05:05 - 0:01:05 - Average dev loss is 0.8607833385467529 at epoch 34
INFO - 03/12/24 22:05:06 - 0:01:05 - dev accuracy is 71.4286 at epoch 34
INFO - 03/12/24 22:05:06 - 0:01:05 - dev average edit distance is 0.8175 at epoch 34
INFO - 03/12/24 22:05:06 - 0:01:05 - At 35-th epoch with lr 0.000639.
INFO - 03/12/24 22:05:07 - 0:01:07 - Running average train loss is 0.7437400477273124 at epoch 35
INFO - 03/12/24 22:05:07 - 0:01:07 - At 36-th epoch with lr 0.000630.
INFO - 03/12/24 22:05:09 - 0:01:08 - Running average train loss is 0.7413385595594134 at epoch 36
INFO - 03/12/24 22:05:09 - 0:01:08 - Average dev loss is 0.8619012236595154 at epoch 36
INFO - 03/12/24 22:05:09 - 0:01:09 - dev accuracy is 73.8095 at epoch 36
INFO - 03/12/24 22:05:09 - 0:01:09 - dev average edit distance is 0.746 at epoch 36
INFO - 03/12/24 22:05:10 - 0:01:09 - At 37-th epoch with lr 0.000621.
INFO - 03/12/24 22:05:11 - 0:01:10 - Running average train loss is 0.7383123976843697 at epoch 37
INFO - 03/12/24 22:05:11 - 0:01:10 - At 38-th epoch with lr 0.000613.
INFO - 03/12/24 22:05:12 - 0:01:12 - Running average train loss is 0.7325258510453361 at epoch 38
INFO - 03/12/24 22:05:12 - 0:01:12 - Average dev loss is 0.85975182056427 at epoch 38
INFO - 03/12/24 22:05:13 - 0:01:12 - dev accuracy is 70.6349 at epoch 38
INFO - 03/12/24 22:05:13 - 0:01:12 - dev average edit distance is 0.8651 at epoch 38
INFO - 03/12/24 22:05:13 - 0:01:13 - At 39-th epoch with lr 0.000605.
INFO - 03/12/24 22:05:15 - 0:01:14 - Running average train loss is 0.7349728090422494 at epoch 39
INFO - 03/12/24 22:05:15 - 0:01:14 - At 40-th epoch with lr 0.000598.
INFO - 03/12/24 22:05:16 - 0:01:15 - Running average train loss is 0.7356613193239484 at epoch 40
INFO - 03/12/24 22:05:16 - 0:01:15 - Average dev loss is 0.8663739562034607 at epoch 40
INFO - 03/12/24 22:05:16 - 0:01:16 - dev accuracy is 75.3968 at epoch 40
INFO - 03/12/24 22:05:16 - 0:01:16 - dev average edit distance is 0.7063 at epoch 40
INFO - 03/12/24 22:05:17 - 0:01:16 - At 41-th epoch with lr 0.000590.
INFO - 03/12/24 22:05:18 - 0:01:18 - Running average train loss is 0.7347638096128192 at epoch 41
INFO - 03/12/24 22:05:18 - 0:01:18 - At 42-th epoch with lr 0.000583.
INFO - 03/12/24 22:05:19 - 0:01:19 - Running average train loss is 0.7324693032673427 at epoch 42
INFO - 03/12/24 22:05:20 - 0:01:19 - Average dev loss is 0.8467552065849304 at epoch 42
INFO - 03/12/24 22:05:20 - 0:01:19 - dev accuracy is 74.6032 at epoch 42
INFO - 03/12/24 22:05:20 - 0:01:19 - dev average edit distance is 0.6905 at epoch 42
INFO - 03/12/24 22:05:20 - 0:01:20 - At 43-th epoch with lr 0.000576.
INFO - 03/12/24 22:05:22 - 0:01:21 - Running average train loss is 0.7289418833596366 at epoch 43
INFO - 03/12/24 22:05:22 - 0:01:21 - At 44-th epoch with lr 0.000570.
INFO - 03/12/24 22:05:23 - 0:01:23 - Running average train loss is 0.7283831238746643 at epoch 44
INFO - 03/12/24 22:05:23 - 0:01:23 - Average dev loss is 0.8489395380020142 at epoch 44
INFO - 03/12/24 22:05:23 - 0:01:23 - dev accuracy is 73.8095 at epoch 44
INFO - 03/12/24 22:05:23 - 0:01:23 - dev average edit distance is 0.7857 at epoch 44
INFO - 03/12/24 22:05:24 - 0:01:23 - At 45-th epoch with lr 0.000563.
INFO - 03/12/24 22:05:25 - 0:01:25 - Running average train loss is 0.7253755501338414 at epoch 45
INFO - 03/12/24 22:05:25 - 0:01:25 - At 46-th epoch with lr 0.000557.
INFO - 03/12/24 22:05:27 - 0:01:26 - Running average train loss is 0.7249315551349095 at epoch 46
INFO - 03/12/24 22:05:27 - 0:01:26 - Average dev loss is 0.8517184257507324 at epoch 46
INFO - 03/12/24 22:05:27 - 0:01:26 - dev accuracy is 73.8095 at epoch 46
INFO - 03/12/24 22:05:27 - 0:01:26 - dev average edit distance is 0.7143 at epoch 46
INFO - 03/12/24 22:05:27 - 0:01:27 - At 47-th epoch with lr 0.000551.
INFO - 03/12/24 22:05:29 - 0:01:28 - Running average train loss is 0.7227065222603934 at epoch 47
INFO - 03/12/24 22:05:29 - 0:01:28 - At 48-th epoch with lr 0.000546.
INFO - 03/12/24 22:05:30 - 0:01:30 - Running average train loss is 0.7226248383522034 at epoch 48
INFO - 03/12/24 22:05:30 - 0:01:30 - Average dev loss is 0.8444769978523254 at epoch 48
INFO - 03/12/24 22:05:30 - 0:01:30 - dev accuracy is 75.3968 at epoch 48
INFO - 03/12/24 22:05:30 - 0:01:30 - dev average edit distance is 0.7222 at epoch 48
INFO - 03/12/24 22:05:31 - 0:01:30 - At 49-th epoch with lr 0.000540.
INFO - 03/12/24 22:05:32 - 0:01:32 - Running average train loss is 0.7191657764571053 at epoch 49
INFO - 03/12/24 22:05:32 - 0:01:32 - At 50-th epoch with lr 0.000535.
INFO - 03/12/24 22:05:34 - 0:01:33 - Running average train loss is 0.71762729542596 at epoch 50
INFO - 03/12/24 22:05:34 - 0:01:33 - Average dev loss is 0.8548915386199951 at epoch 50
INFO - 03/12/24 22:05:34 - 0:01:34 - dev accuracy is 74.6032 at epoch 50
INFO - 03/12/24 22:05:34 - 0:01:34 - dev average edit distance is 0.746 at epoch 50
INFO - 03/12/24 22:05:34 - 0:01:34 - At 51-th epoch with lr 0.000529.
INFO - 03/12/24 22:05:36 - 0:01:35 - Running average train loss is 0.7171942506517682 at epoch 51
INFO - 03/12/24 22:05:36 - 0:01:35 - At 52-th epoch with lr 0.000524.
INFO - 03/12/24 22:05:37 - 0:01:37 - Running average train loss is 0.7154166102409363 at epoch 52
INFO - 03/12/24 22:05:37 - 0:01:37 - Average dev loss is 0.8351304531097412 at epoch 52
INFO - 03/12/24 22:05:38 - 0:01:37 - dev accuracy is 77.7778 at epoch 52
INFO - 03/12/24 22:05:38 - 0:01:37 - dev average edit distance is 0.7619 at epoch 52
INFO - 03/12/24 22:05:38 - 0:01:38 - At 53-th epoch with lr 0.000519.
INFO - 03/12/24 22:05:40 - 0:01:39 - Running average train loss is 0.71462596314294 at epoch 53
INFO - 03/12/24 22:05:40 - 0:01:39 - At 54-th epoch with lr 0.000514.
INFO - 03/12/24 22:05:41 - 0:01:40 - Running average train loss is 0.7152220181056431 at epoch 54
INFO - 03/12/24 22:05:41 - 0:01:41 - Average dev loss is 0.8489721417427063 at epoch 54
INFO - 03/12/24 22:05:41 - 0:01:41 - dev accuracy is 77.7778 at epoch 54
INFO - 03/12/24 22:05:41 - 0:01:41 - dev average edit distance is 0.627 at epoch 54
INFO - 03/12/24 22:05:42 - 0:01:41 - At 55-th epoch with lr 0.000510.
INFO - 03/12/24 22:05:43 - 0:01:43 - Running average train loss is 0.7140361070632935 at epoch 55
INFO - 03/12/24 22:05:43 - 0:01:43 - At 56-th epoch with lr 0.000505.
INFO - 03/12/24 22:05:45 - 0:01:44 - Running average train loss is 0.7109444226537432 at epoch 56
INFO - 03/12/24 22:05:45 - 0:01:44 - Average dev loss is 0.8439702987670898 at epoch 56
INFO - 03/12/24 22:05:45 - 0:01:44 - dev accuracy is 77.7778 at epoch 56
INFO - 03/12/24 22:05:45 - 0:01:44 - dev average edit distance is 0.7143 at epoch 56
INFO - 03/12/24 22:05:45 - 0:01:45 - At 57-th epoch with lr 0.000501.
INFO - 03/12/24 22:05:47 - 0:01:46 - Running average train loss is 0.7112144657543727 at epoch 57
INFO - 03/12/24 22:05:47 - 0:01:46 - At 58-th epoch with lr 0.000496.
INFO - 03/12/24 22:05:48 - 0:01:48 - Running average train loss is 0.7112746153559003 at epoch 58
INFO - 03/12/24 22:05:48 - 0:01:48 - Average dev loss is 0.8426886796951294 at epoch 58
INFO - 03/12/24 22:05:48 - 0:01:48 - dev accuracy is 77.7778 at epoch 58
INFO - 03/12/24 22:05:48 - 0:01:48 - dev average edit distance is 0.6429 at epoch 58
INFO - 03/12/24 22:05:49 - 0:01:48 - At 59-th epoch with lr 0.000492.
INFO - 03/12/24 22:05:50 - 0:01:50 - Running average train loss is 0.7105990307671683 at epoch 59
INFO - 03/12/24 22:05:50 - 0:01:50 - At 60-th epoch with lr 0.000488.
INFO - 03/12/24 22:05:52 - 0:01:51 - Running average train loss is 0.7081397005489894 at epoch 60
INFO - 03/12/24 22:05:52 - 0:01:51 - Average dev loss is 0.8354730606079102 at epoch 60
INFO - 03/12/24 22:05:52 - 0:01:51 - dev accuracy is 77.7778 at epoch 60
INFO - 03/12/24 22:05:52 - 0:01:51 - dev average edit distance is 0.6667 at epoch 60
INFO - 03/12/24 22:05:52 - 0:01:52 - At 61-th epoch with lr 0.000484.
INFO - 03/12/24 22:05:54 - 0:01:53 - Running average train loss is 0.7089474286351886 at epoch 61
INFO - 03/12/24 22:05:54 - 0:01:53 - At 62-th epoch with lr 0.000480.
INFO - 03/12/24 22:05:55 - 0:01:55 - Running average train loss is 0.70611629315785 at epoch 62
INFO - 03/12/24 22:05:55 - 0:01:55 - Average dev loss is 0.8514763116836548 at epoch 62
INFO - 03/12/24 22:05:55 - 0:01:55 - dev accuracy is 76.1905 at epoch 62
INFO - 03/12/24 22:05:55 - 0:01:55 - dev average edit distance is 0.7222 at epoch 62
INFO - 03/12/24 22:05:56 - 0:01:55 - At 63-th epoch with lr 0.000476.
INFO - 03/12/24 22:05:57 - 0:01:57 - Running average train loss is 0.7079109038625445 at epoch 63
INFO - 03/12/24 22:05:57 - 0:01:57 - At 64-th epoch with lr 0.000472.
INFO - 03/12/24 22:05:59 - 0:01:58 - Running average train loss is 0.7085562348365784 at epoch 64
INFO - 03/12/24 22:05:59 - 0:01:58 - Average dev loss is 0.8474122285842896 at epoch 64
INFO - 03/12/24 22:05:59 - 0:01:58 - dev accuracy is 78.5714 at epoch 64
INFO - 03/12/24 22:05:59 - 0:01:58 - dev average edit distance is 0.6349 at epoch 64
INFO - 03/12/24 22:05:59 - 0:01:59 - At 65-th epoch with lr 0.000469.
INFO - 03/12/24 22:06:01 - 0:02:00 - Running average train loss is 0.7071496503693717 at epoch 65
INFO - 03/12/24 22:06:01 - 0:02:00 - At 66-th epoch with lr 0.000465.
INFO - 03/12/24 22:06:02 - 0:02:02 - Running average train loss is 0.7081728237015861 at epoch 66
INFO - 03/12/24 22:06:02 - 0:02:02 - Average dev loss is 0.8350522518157959 at epoch 66
INFO - 03/12/24 22:06:02 - 0:02:02 - dev accuracy is 76.9841 at epoch 66
INFO - 03/12/24 22:06:02 - 0:02:02 - dev average edit distance is 0.7143 at epoch 66
INFO - 03/12/24 22:06:03 - 0:02:02 - At 67-th epoch with lr 0.000462.
INFO - 03/12/24 22:06:04 - 0:02:04 - Running average train loss is 0.703141987323761 at epoch 67
INFO - 03/12/24 22:06:04 - 0:02:04 - At 68-th epoch with lr 0.000458.
INFO - 03/12/24 22:06:06 - 0:02:05 - Running average train loss is 0.7025609357016427 at epoch 68
INFO - 03/12/24 22:06:06 - 0:02:05 - Average dev loss is 0.8501496315002441 at epoch 68
INFO - 03/12/24 22:06:06 - 0:02:05 - dev accuracy is 76.9841 at epoch 68
INFO - 03/12/24 22:06:06 - 0:02:05 - dev average edit distance is 0.6587 at epoch 68
INFO - 03/12/24 22:06:06 - 0:02:06 - At 69-th epoch with lr 0.000455.
INFO - 03/12/24 22:06:08 - 0:02:07 - Running average train loss is 0.7037883230618068 at epoch 69
INFO - 03/12/24 22:06:08 - 0:02:07 - At 70-th epoch with lr 0.000452.
INFO - 03/12/24 22:06:09 - 0:02:09 - Running average train loss is 0.7023341315133231 at epoch 70
INFO - 03/12/24 22:06:09 - 0:02:09 - Average dev loss is 0.8559889197349548 at epoch 70
INFO - 03/12/24 22:06:09 - 0:02:09 - dev accuracy is 73.0159 at epoch 70
INFO - 03/12/24 22:06:09 - 0:02:09 - dev average edit distance is 0.7222 at epoch 70
INFO - 03/12/24 22:06:10 - 0:02:09 - At 71-th epoch with lr 0.000449.
INFO - 03/12/24 22:06:11 - 0:02:11 - Running average train loss is 0.7014521445546832 at epoch 71
INFO - 03/12/24 22:06:11 - 0:02:11 - At 72-th epoch with lr 0.000445.
INFO - 03/12/24 22:06:13 - 0:02:12 - Running average train loss is 0.7007253766059875 at epoch 72
INFO - 03/12/24 22:06:13 - 0:02:12 - Average dev loss is 0.8563520908355713 at epoch 72
INFO - 03/12/24 22:06:13 - 0:02:12 - dev accuracy is 72.2222 at epoch 72
INFO - 03/12/24 22:06:13 - 0:02:12 - dev average edit distance is 0.746 at epoch 72
INFO - 03/12/24 22:06:13 - 0:02:13 - At 73-th epoch with lr 0.000442.
INFO - 03/12/24 22:06:15 - 0:02:14 - Running average train loss is 0.6987377575465611 at epoch 73
INFO - 03/12/24 22:06:15 - 0:02:14 - At 74-th epoch with lr 0.000439.
INFO - 03/12/24 22:06:16 - 0:02:16 - Running average train loss is 0.6987947566168649 at epoch 74
INFO - 03/12/24 22:06:16 - 0:02:16 - Average dev loss is 0.8314708471298218 at epoch 74
INFO - 03/12/24 22:06:17 - 0:02:16 - dev accuracy is 77.7778 at epoch 74
INFO - 03/12/24 22:06:17 - 0:02:16 - dev average edit distance is 0.746 at epoch 74
INFO - 03/12/24 22:06:17 - 0:02:16 - At 75-th epoch with lr 0.000436.
INFO - 03/12/24 22:06:18 - 0:02:18 - Running average train loss is 0.6973937153816223 at epoch 75
INFO - 03/12/24 22:06:18 - 0:02:18 - At 76-th epoch with lr 0.000434.
INFO - 03/12/24 22:06:20 - 0:02:19 - Running average train loss is 0.6977671895708356 at epoch 76
INFO - 03/12/24 22:06:20 - 0:02:19 - Average dev loss is 0.8414874076843262 at epoch 76
INFO - 03/12/24 22:06:20 - 0:02:20 - dev accuracy is 76.9841 at epoch 76
INFO - 03/12/24 22:06:20 - 0:02:20 - dev average edit distance is 0.7381 at epoch 76
INFO - 03/12/24 22:06:20 - 0:02:20 - At 77-th epoch with lr 0.000431.
INFO - 03/12/24 22:06:22 - 0:02:21 - Running average train loss is 0.6975246156964984 at epoch 77
INFO - 03/12/24 22:06:22 - 0:02:21 - At 78-th epoch with lr 0.000428.
INFO - 03/12/24 22:06:23 - 0:02:23 - Running average train loss is 0.6953684602464948 at epoch 78
INFO - 03/12/24 22:06:23 - 0:02:23 - Average dev loss is 0.8518800735473633 at epoch 78
INFO - 03/12/24 22:06:24 - 0:02:23 - dev accuracy is 73.0159 at epoch 78
INFO - 03/12/24 22:06:24 - 0:02:23 - dev average edit distance is 0.7381 at epoch 78
INFO - 03/12/24 22:06:24 - 0:02:24 - At 79-th epoch with lr 0.000425.
INFO - 03/12/24 22:06:25 - 0:02:25 - Running average train loss is 0.6954266428947449 at epoch 79
INFO - 03/12/24 22:06:25 - 0:02:25 - At 80-th epoch with lr 0.000423.
INFO - 03/12/24 22:06:27 - 0:02:26 - Running average train loss is 0.6964525154658726 at epoch 80
INFO - 03/12/24 22:06:27 - 0:02:26 - Average dev loss is 0.844744861125946 at epoch 80
INFO - 03/12/24 22:06:27 - 0:02:27 - dev accuracy is 75.3968 at epoch 80
INFO - 03/12/24 22:06:27 - 0:02:27 - dev average edit distance is 0.7143 at epoch 80
INFO - 03/12/24 22:06:28 - 0:02:27 - At 81-th epoch with lr 0.000420.
INFO - 03/12/24 22:06:29 - 0:02:29 - Running average train loss is 0.6950409071786063 at epoch 81
INFO - 03/12/24 22:06:29 - 0:02:29 - At 82-th epoch with lr 0.000417.
INFO - 03/12/24 22:06:30 - 0:02:30 - Running average train loss is 0.6938304645674569 at epoch 82
INFO - 03/12/24 22:06:30 - 0:02:30 - Average dev loss is 0.8494565486907959 at epoch 82
INFO - 03/12/24 22:06:31 - 0:02:30 - dev accuracy is 76.1905 at epoch 82
INFO - 03/12/24 22:06:31 - 0:02:30 - dev average edit distance is 0.6825 at epoch 82
INFO - 03/12/24 22:06:31 - 0:02:31 - At 83-th epoch with lr 0.000415.
INFO - 03/12/24 22:06:33 - 0:02:32 - Running average train loss is 0.6945515785898481 at epoch 83
INFO - 03/12/24 22:06:33 - 0:02:32 - At 84-th epoch with lr 0.000412.
INFO - 03/12/24 22:06:34 - 0:02:34 - Running average train loss is 0.694594715322767 at epoch 84
INFO - 03/12/24 22:06:34 - 0:02:34 - Average dev loss is 0.8451783657073975 at epoch 84
INFO - 03/12/24 22:06:34 - 0:02:34 - dev accuracy is 80.1587 at epoch 84
INFO - 03/12/24 22:06:34 - 0:02:34 - dev average edit distance is 0.6111 at epoch 84
INFO - 03/12/24 22:06:35 - 0:02:34 - At 85-th epoch with lr 0.000410.
INFO - 03/12/24 22:06:36 - 0:02:36 - Running average train loss is 0.696792449269976 at epoch 85
INFO - 03/12/24 22:06:36 - 0:02:36 - At 86-th epoch with lr 0.000408.
INFO - 03/12/24 22:06:38 - 0:02:37 - Running average train loss is 0.6938738397189549 at epoch 86
INFO - 03/12/24 22:06:38 - 0:02:37 - Average dev loss is 0.8483597636222839 at epoch 86
INFO - 03/12/24 22:06:38 - 0:02:38 - dev accuracy is 76.9841 at epoch 86
INFO - 03/12/24 22:06:38 - 0:02:38 - dev average edit distance is 0.619 at epoch 86
INFO - 03/12/24 22:06:38 - 0:02:38 - At 87-th epoch with lr 0.000405.
INFO - 03/12/24 22:06:40 - 0:02:39 - Running average train loss is 0.69464510679245 at epoch 87
INFO - 03/12/24 22:06:40 - 0:02:39 - At 88-th epoch with lr 0.000403.
INFO - 03/12/24 22:06:41 - 0:02:41 - Running average train loss is 0.6924145306859698 at epoch 88
INFO - 03/12/24 22:06:41 - 0:02:41 - Average dev loss is 0.8585017919540405 at epoch 88
INFO - 03/12/24 22:06:42 - 0:02:41 - dev accuracy is 73.0159 at epoch 88
INFO - 03/12/24 22:06:42 - 0:02:41 - dev average edit distance is 0.7302 at epoch 88
INFO - 03/12/24 22:06:42 - 0:02:42 - At 89-th epoch with lr 0.000401.
INFO - 03/12/24 22:06:43 - 0:02:43 - Running average train loss is 0.6932242597852435 at epoch 89
INFO - 03/12/24 22:06:43 - 0:02:43 - At 90-th epoch with lr 0.000398.
INFO - 03/12/24 22:06:45 - 0:02:44 - Running average train loss is 0.6922885690416608 at epoch 90
INFO - 03/12/24 22:06:45 - 0:02:44 - Average dev loss is 0.8652627468109131 at epoch 90
INFO - 03/12/24 22:06:45 - 0:02:45 - dev accuracy is 72.2222 at epoch 90
INFO - 03/12/24 22:06:45 - 0:02:45 - dev average edit distance is 0.754 at epoch 90
INFO - 03/12/24 22:06:46 - 0:02:45 - At 91-th epoch with lr 0.000396.
INFO - 03/12/24 22:06:47 - 0:02:46 - Running average train loss is 0.6921145830835614 at epoch 91
INFO - 03/12/24 22:06:47 - 0:02:46 - At 92-th epoch with lr 0.000394.
INFO - 03/12/24 22:06:48 - 0:02:48 - Running average train loss is 0.6928055797304425 at epoch 92
INFO - 03/12/24 22:06:48 - 0:02:48 - Average dev loss is 0.8447373509407043 at epoch 92
INFO - 03/12/24 22:06:49 - 0:02:48 - dev accuracy is 77.7778 at epoch 92
INFO - 03/12/24 22:06:49 - 0:02:48 - dev average edit distance is 0.6984 at epoch 92
INFO - 03/12/24 22:06:49 - 0:02:49 - At 93-th epoch with lr 0.000392.
INFO - 03/12/24 22:06:51 - 0:02:50 - Running average train loss is 0.6912180185317993 at epoch 93
INFO - 03/12/24 22:06:51 - 0:02:50 - At 94-th epoch with lr 0.000390.
INFO - 03/12/24 22:06:52 - 0:02:52 - Running average train loss is 0.6912129521369934 at epoch 94
INFO - 03/12/24 22:06:52 - 0:02:52 - Average dev loss is 0.8413613438606262 at epoch 94
INFO - 03/12/24 22:06:52 - 0:02:52 - dev accuracy is 78.5714 at epoch 94
INFO - 03/12/24 22:06:52 - 0:02:52 - dev average edit distance is 0.6349 at epoch 94
INFO - 03/12/24 22:06:53 - 0:02:52 - At 95-th epoch with lr 0.000388.
INFO - 03/12/24 22:06:54 - 0:02:54 - Running average train loss is 0.6911626202719552 at epoch 95
INFO - 03/12/24 22:06:54 - 0:02:54 - At 96-th epoch with lr 0.000386.
INFO - 03/12/24 22:06:56 - 0:02:55 - Running average train loss is 0.6907351272446769 at epoch 96
INFO - 03/12/24 22:06:56 - 0:02:55 - Average dev loss is 0.8407472968101501 at epoch 96
INFO - 03/12/24 22:06:56 - 0:02:56 - dev accuracy is 77.7778 at epoch 96
INFO - 03/12/24 22:06:56 - 0:02:56 - dev average edit distance is 0.6667 at epoch 96
INFO - 03/12/24 22:06:57 - 0:02:56 - At 97-th epoch with lr 0.000384.
INFO - 03/12/24 22:06:58 - 0:02:58 - Running average train loss is 0.6907394783837455 at epoch 97
INFO - 03/12/24 22:06:58 - 0:02:58 - At 98-th epoch with lr 0.000382.
INFO - 03/12/24 22:07:00 - 0:02:59 - Running average train loss is 0.6904835615839277 at epoch 98
INFO - 03/12/24 22:07:00 - 0:02:59 - Average dev loss is 0.8533101081848145 at epoch 98
INFO - 03/12/24 22:07:00 - 0:02:59 - dev accuracy is 77.7778 at epoch 98
INFO - 03/12/24 22:07:00 - 0:02:59 - dev average edit distance is 0.6746 at epoch 98
INFO - 03/12/24 22:07:00 - 0:03:00 - At 99-th epoch with lr 0.000380.
INFO - 03/12/24 22:07:02 - 0:03:01 - Running average train loss is 0.6904658164296832 at epoch 99
INFO - 03/12/24 22:07:02 - 0:03:01 - Average dev loss is 0.8543862700462341 at epoch 99
INFO - 03/12/24 22:07:02 - 0:03:02 - dev accuracy is 76.9841 at epoch 99
INFO - 03/12/24 22:07:02 - 0:03:02 - dev average edit distance is 0.6587 at epoch 99
INFO - 03/12/24 22:07:03 - 0:03:02 - loading checkpoints/sig22/tagtransformer/xty.nll_0.8452.acc_80.1587.dist_0.6111.epoch_84 for testing
INFO - 03/12/24 22:07:03 - 0:03:02 - load model in checkpoints/sig22/tagtransformer/xty.nll_0.8452.acc_80.1587.dist_0.6111.epoch_84
INFO - 03/12/24 22:07:03 - 0:03:02 - Average dev loss is 0.8451783657073975 at epoch -1
INFO - 03/12/24 22:07:03 - 0:03:02 - decoding dev set
INFO - 03/12/24 22:07:03 - 0:03:03 - finished decoding 126 dev instance
INFO - 03/12/24 22:07:03 - 0:03:03 - DEV accuracy is 80.1587 at epoch -1
INFO - 03/12/24 22:07:03 - 0:03:03 - DEV average edit distance is 0.6111 at epoch -1
INFO - 03/12/24 22:07:03 - 0:03:03 - DEV xty acc 80.1587 dist 0.6111
INFO - 03/12/24 22:07:03 - 0:03:03 - decoding test set
INFO - 03/12/24 22:07:03 - 0:03:03 - finished decoding 253 test instance
INFO - 03/12/24 22:07:03 - 0:03:03 - TEST accuracy is 0.0 at epoch -1
INFO - 03/12/24 22:07:03 - 0:03:03 - TEST average edit distance is 11.668 at epoch -1
INFO - 03/12/24 22:07:03 - 0:03:03 - TEST xty acc 0.0 dist 11.668
TRAINING DONE!!
FINAL SUBMISSION DONE!!
